{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial on building Deep learning models using Keras toolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jcpK31fbfPZk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1143\n"
     ]
    }
   ],
   "source": [
    "#read input\n",
    "f=open('Dataset/sentiment.txt','r')\n",
    "lines=f.readlines()\n",
    "f.close()\n",
    "print (len(lines))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DW8DT4e8fpb0"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "\n",
    "stopwords = [\"a\", \"about\", \"above\", \"above\", \"across\", \"after\", \"afterwards\", \"again\", \"against\", \"all\", \"almost\", \"alone\", \"along\", \"already\", \"also\",\"although\",\"always\",\"am\",\"among\", \"amongst\", \"amoungst\", \"amount\",  \"an\", \"and\", \"another\", \"any\",\"anyhow\",\"anyone\",\"anything\",\"anyway\", \"anywhere\", \"are\", \"around\", \"as\",  \"at\", \"back\",\"be\",\"became\", \"because\",\"become\",\"becomes\", \"becoming\", \"been\", \"before\", \"beforehand\", \"behind\", \"being\", \"below\", \"beside\", \"besides\", \"between\", \"beyond\", \"bill\", \"both\", \"bottom\",\"but\", \"by\", \"call\", \"can\", \"cannot\", \"cant\", \"co\", \"con\", \"could\", \"couldnt\", \"cry\", \"de\", \"describe\", \"detail\", \"do\", \"done\", \"down\", \"due\", \"during\", \"each\", \"eg\", \"eight\", \"either\", \"eleven\",\"else\", \"elsewhere\", \"empty\", \"enough\", \"etc\", \"even\", \"ever\", \"every\", \"everyone\", \"everything\", \"everywhere\", \"except\", \"few\", \"fifteen\", \"fify\", \"fill\", \"find\", \"fire\", \"first\", \"five\", \"for\", \"former\", \"formerly\", \"forty\", \"found\", \"four\", \"from\", \"front\", \"full\", \"further\", \"get\", \"give\", \"go\", \"had\", \"has\", \"hasnt\", \"have\", \"he\", \"hence\", \"her\", \"here\", \"hereafter\", \"hereby\", \"herein\", \"hereupon\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"however\", \"hundred\", \"ie\", \"if\", \"in\", \"inc\", \"indeed\", \"interest\", \"into\", \"is\", \"it\", \"its\", \"itself\", \"keep\", \"last\", \"latter\", \"latterly\", \"least\", \"less\", \"ltd\", \"made\", \"many\", \"may\", \"me\", \"meanwhile\", \"might\", \"mill\", \"mine\", \"more\", \"moreover\", \"most\", \"mostly\", \"move\", \"much\", \"must\", \"my\", \"myself\", \"name\", \"namely\", \"neither\", \"never\", \"nevertheless\", \"next\", \"nine\", \"no\", \"nobody\", \"none\", \"noone\", \"nor\", \"not\", \"nothing\", \"now\", \"nowhere\", \"of\", \"off\", \"often\", \"on\", \"once\", \"one\", \"only\", \"onto\", \"or\", \"other\", \"others\", \"otherwise\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\",\"part\", \"per\", \"perhaps\", \"please\", \"put\", \"rather\", \"re\", \"same\", \"see\", \"seem\", \"seemed\", \"seeming\", \"seems\", \"serious\", \"several\", \"she\", \"should\", \"show\", \"side\", \"since\", \"sincere\", \"six\", \"sixty\", \"so\", \"some\", \"somehow\", \"someone\", \"something\", \"sometime\", \"sometimes\", \"somewhere\", \"still\", \"such\", \"system\", \"take\", \"ten\", \"than\", \"that\", \"the\", \"their\", \"them\", \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\", \"therefore\", \"therein\", \"thereupon\", \"these\", \"they\", \"thickv\", \"thin\", \"third\", \"this\", \"those\", \"though\", \"three\", \"through\", \"throughout\", \"thru\", \"thus\", \"to\", \"together\", \"too\", \"top\", \"toward\", \"towards\", \"twelve\", \"twenty\", \"two\", \"un\", \"under\", \"until\", \"up\", \"upon\", \"us\", \"very\", \"via\", \"was\", \"we\", \"well\", \"were\", \"what\", \"whatever\", \"when\", \"whence\", \"whenever\", \"where\", \"whereafter\", \"whereas\", \"whereby\", \"wherein\", \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\", \"whither\", \"who\", \"whoever\", \"whole\", \"whom\", \"whose\", \"why\", \"will\", \"with\", \"within\", \"without\", \"would\", \"yet\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"the\"]\n",
    "\n",
    "def text_cleaner(text): \n",
    "    text=remove_link(text.lower())\n",
    "    long_words=[]\n",
    "    for i in text.split():\n",
    "        if i not in stopwords:                  \n",
    "            long_words.append(i)\n",
    "    return long_words\n",
    "\n",
    "def remove_link(text):\n",
    "    regex = r'https?://[^\\s<>)\"‘’]+'\n",
    "    match = re.sub(regex,' ', text)\n",
    "    regex = r'https?:|urls?|[/\\:,-.\"\\'?!;…]+'\n",
    "    tweet = re.sub(regex,' ', match)\n",
    "    tweet = re.sub(\"[^a-zA-Z_]\", \" \", tweet)\n",
    "    tweet = re.sub(\"[ ]+\", \" \", tweet) \n",
    "    return tweet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6iDUew_bqEYT"
   },
   "source": [
    "**Cross validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "HaoTjGtwfxBb",
    "outputId": "e6166005-52f3-4027-a768-c178d374d0ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size :  914\n",
      "Test set size :  229\n"
     ]
    }
   ],
   "source": [
    "#divide the data into train set and test set\n",
    "import random\n",
    "random.seed(1)\n",
    "random.shuffle(lines) #shuffle the dataset before dividing it into train and test set\n",
    "split_size = int(0.8*len(lines)) #use 80% of total data as train set and 20% as test set\n",
    "train_lines = lines[:split_size]\n",
    "test_lines = lines[split_size:]\n",
    "\n",
    "print (\"Training set size : \", len(train_lines))\n",
    "print (\"Test set size : \", len(test_lines))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "DhD9MhKYf2Wz",
    "outputId": "f5fdb3e1-eae7-4d43-e912-15d48ae339d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size :  4012\n",
      "Max sentence length :  19\n"
     ]
    }
   ],
   "source": [
    "#convert string tokens to integers\n",
    "#create a vocabulary set and assign a unique id to each word in the vocabulary\n",
    "\n",
    "#load all unique vocabulary\n",
    "vocab = []\n",
    "maxlen = []\n",
    "for l in train_lines:\n",
    "    s = l.strip().split('\\t')\n",
    "    words = text_cleaner(s[0].strip())\n",
    "    vocab += words\n",
    "    maxlen.append(len(words))\n",
    "vocab = list(set(vocab))\n",
    "print (\"Vocabulary size : \", len(vocab))\n",
    "\n",
    "#assign unique id to each vocabulary\n",
    "word2id = dict()\n",
    "for i,v in enumerate(vocab,1):\n",
    "    word2id[v] = i\n",
    "word2id['PAD'] = 0 #special token to take care of unseen words in the test set\n",
    "maxlen = max(maxlen)\n",
    "print (\"Max sentence length : \",maxlen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "SEdRzE28gPtC",
    "outputId": "9437ef6e-f592-4c2c-a688-60849334ddfd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "914\n",
      "914\n",
      "[3218, 3130, 3341, 1089, 423, 2087, 2087, 3025, 1384, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 0]\n"
     ]
    }
   ],
   "source": [
    "#Prepare train and test set\n",
    "#Convert strings to integers\n",
    "#prepare train and test set\n",
    "\n",
    "#prepare train set\n",
    "import numpy as np\n",
    "train_X = []\n",
    "train_Y = []\n",
    "for l in train_lines:\n",
    "    s = l.strip().split('\\t')\n",
    "    text = text_cleaner(s[0])\n",
    "    label = int(s[1].strip())\n",
    "    temp_x = [word2id[x] for x in text]\n",
    "    temp_x += [0]* (maxlen-len(temp_x)) #convert all input to equal size to enable training in batches\n",
    "    temp_y = [0]*2\n",
    "    temp_y[label] = 1\n",
    "    train_X.append(temp_x)\n",
    "    train_Y.append(temp_y)\n",
    "print (len(train_X))\n",
    "print (len(train_Y))\n",
    "print (train_X[0])\n",
    "print (train_Y[0])\n",
    "train_X = np.array(train_X)\n",
    "train_Y = np.array(train_Y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "0AAqcptJgjCw",
    "outputId": "2411580f-8670-46a4-c33f-8c60b8863d2c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "229\n",
      "229\n",
      "[1060, 3396, 2791, 3651, 0, 0, 837, 28, 2134, 1384, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 0]\n"
     ]
    }
   ],
   "source": [
    "#prepare test set\n",
    "test_X = []\n",
    "test_Y = []\n",
    "\n",
    "for l in test_lines:\n",
    "    s = l.strip().split('\\t')\n",
    "    text = text_cleaner(s[0].strip())\n",
    "    label = int(s[1].strip())\n",
    "    temp_x = [word2id[x] if x in word2id else 0 for x in text ][:maxlen]\n",
    "    temp_x += [0]* (maxlen-len(temp_x)) #convert all input to equal size to enable training in batches\n",
    "    temp_y = [0]*2\n",
    "    temp_y[label] = 1\n",
    "    test_X.append(temp_x)\n",
    "    test_Y.append(temp_y)\n",
    "print (len(test_X))\n",
    "print (len(test_Y))\n",
    "print (test_X[0])\n",
    "print (test_Y[0])\n",
    "test_X = np.array(test_X)\n",
    "test_Y = np.array(test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 714
    },
    "colab_type": "code",
    "id": "F7bfOXPEgtHv",
    "outputId": "5e0f3d12-39ec-4068-f641-d12ed356419b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "914/914 [==============================] - 0s 91us/step - loss: 0.9284 - accuracy: 0.5558\n",
      "Epoch 2/20\n",
      "914/914 [==============================] - 0s 32us/step - loss: 0.7879 - accuracy: 0.5853\n",
      "Epoch 3/20\n",
      "914/914 [==============================] - 0s 40us/step - loss: 0.7265 - accuracy: 0.6455\n",
      "Epoch 4/20\n",
      "914/914 [==============================] - 0s 25us/step - loss: 0.7243 - accuracy: 0.6433\n",
      "Epoch 5/20\n",
      "914/914 [==============================] - 0s 35us/step - loss: 0.7294 - accuracy: 0.6280\n",
      "Epoch 6/20\n",
      "914/914 [==============================] - 0s 30us/step - loss: 0.7157 - accuracy: 0.6225\n",
      "Epoch 7/20\n",
      "914/914 [==============================] - 0s 29us/step - loss: 0.6889 - accuracy: 0.6389\n",
      "Epoch 8/20\n",
      "914/914 [==============================] - 0s 33us/step - loss: 0.6650 - accuracy: 0.6444\n",
      "Epoch 9/20\n",
      "914/914 [==============================] - 0s 25us/step - loss: 0.6631 - accuracy: 0.6488\n",
      "Epoch 10/20\n",
      "914/914 [==============================] - 0s 29us/step - loss: 0.6599 - accuracy: 0.6663\n",
      "Epoch 11/20\n",
      "914/914 [==============================] - 0s 28us/step - loss: 0.6564 - accuracy: 0.6554\n",
      "Epoch 12/20\n",
      "914/914 [==============================] - 0s 31us/step - loss: 0.6517 - accuracy: 0.6565\n",
      "Epoch 13/20\n",
      "914/914 [==============================] - 0s 30us/step - loss: 0.6499 - accuracy: 0.6619\n",
      "Epoch 14/20\n",
      "914/914 [==============================] - 0s 25us/step - loss: 0.6460 - accuracy: 0.6641\n",
      "Epoch 15/20\n",
      "914/914 [==============================] - 0s 31us/step - loss: 0.6467 - accuracy: 0.6608\n",
      "Epoch 16/20\n",
      "914/914 [==============================] - 0s 28us/step - loss: 0.6470 - accuracy: 0.6641\n",
      "Epoch 17/20\n",
      "914/914 [==============================] - 0s 28us/step - loss: 0.6438 - accuracy: 0.6674\n",
      "Epoch 18/20\n",
      "914/914 [==============================] - 0s 31us/step - loss: 0.6433 - accuracy: 0.6532\n",
      "Epoch 19/20\n",
      "914/914 [==============================] - 0s 36us/step - loss: 0.6394 - accuracy: 0.6619\n",
      "Epoch 20/20\n",
      "914/914 [==============================] - 0s 32us/step - loss: 0.6369 - accuracy: 0.6652\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f8f63dd5590>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential #Sequential is the class in Keras library that defines a model comprising of linear stack of models\n",
    "from keras.layers import Dense\n",
    "##initialize a sequential model\n",
    "model = Sequential() \n",
    "\n",
    "#Add layers to the sequential model\n",
    "#For the first hidden layer, input_dim must be mentioned which is the dimension of the input that the hidden layer receives from the input layer\n",
    "#The below line adds a hidden layer of 50 nodes that is connected to a input layer with maxlen nodes\n",
    "#tanh activation function applies nonlinear transformation to the output of the hidden layer\n",
    "model.add(Dense(50,input_dim=maxlen,activation='tanh')) \n",
    "\n",
    "#Add output layer with two nodes since our dataset comprises of two classes\n",
    "#Softmax converts the output to a probability distribution.\n",
    "model.add(Dense(2,activation='softmax'))\n",
    "\n",
    "#Compile the model by specifying the optimizers and loss functions\n",
    "model.compile(optimizer='sgd', loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "#train the model\n",
    "model.fit(train_X, train_Y, epochs=20, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "CNuR6-a2g2F3",
    "outputId": "f353f37f-d43a-41a9-bb22-916318a3c3ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 50)                1000      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 102       \n",
      "=================================================================\n",
      "Total params: 1,102\n",
      "Trainable params: 1,102\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "GqQyUkMhhJ-f",
    "outputId": "40a1891f-9f09-43a3-f8e8-6df5ea50956b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "229/229 [==============================] - 0s 123us/step\n"
     ]
    }
   ],
   "source": [
    "res = model.evaluate(test_X,test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "vJzjAGXEhMk5",
    "outputId": "dffe161a-799f-4dd5-cb9f-de5f52eef348"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7072132375563076, 0.5938864350318909]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xTXnV_u1hUYh"
   },
   "outputs": [],
   "source": [
    "p = model.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "udHI6-ikhcXN",
    "outputId": "5ca73a07-8c16-4654-bd17-f0b3f7667a1c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We blame cities for the majority of CO2 emissions without acknowledging their vulnerability to #CFCC15 #journey2015 #S2228 #SemST\t0 [0.81120926 0.18879077]\n",
      "Feminists who go for a gender studies degree should also blame the patriarchy for their mediocre grades in science. #SemST\t0 [0.6729641  0.32703587]\n",
      "Just wrote my blog to help @CalAlimony pass a vital law that ends #alimony. Posting soon. #Divorce #leanin #SemST\t1 [0.90414804 0.09585188]\n",
      "RT @JohnFugelsang: They should just make the GOP primaries a reality game show called \"Who Wants To Get Beat Up By A Girl? #SemST\t0 [0.5836141  0.41638583]\n",
      "It's incredibly easy to identify shitty females with a poor view on the world and what's important thanks to #SemST\t0 [0.58727384 0.4127261 ]\n"
     ]
    }
   ],
   "source": [
    "for i,l in enumerate(test_lines[:5]):\n",
    "  print (l.strip(),p[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o2qVy1VliDz1"
   },
   "outputs": [],
   "source": [
    "#Generate word2vec embeddings using gensim\n",
    "import gensim, logging, os\n",
    "from gensim import corpora\n",
    "from collections import defaultdict\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "t1 = time.time()\n",
    "texts = [[word for word in text_cleaner(line.strip().split('\\t')[0])]  for line in open('Dataset/sentiment.txt')]\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "model = gensim.models.Word2Vec(texts, min_count=0,sample=0.001, seed=1, workers=8, min_alpha=0.0001, sg=0, hs=0, negative=5,iter=100,size=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "weWASEJ1iXHs",
    "outputId": "ddece87e-a296-421e-c4ac-b5d6b54f163a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gyanendro/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time taken  4.447230100631714\n"
     ]
    }
   ],
   "source": [
    "##word2vec model trained\n",
    "##Now dump the generated embeddings into text files\n",
    "f2=open('Embeddings/words.txt','w')\n",
    "X = []\n",
    "for key, value in dictionary.token2id.items() :\n",
    "    #print (key, value)\n",
    "    f2.write(key+\"\\n\")\n",
    "    X.append(list(model[key]))\n",
    "f2.close()\n",
    "\n",
    "fp=open('Embeddings/vectors.txt','w')\n",
    "for w in list(X):\n",
    "    x=list(w)\n",
    "    for wi in x:\n",
    "        fp.write(str(wi).replace('\\n','')+'\\t')\n",
    "    fp.write('\\n')\n",
    "fp.close()\n",
    "t2 = time.time()\n",
    "print (\"Total time taken \",t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xTYsTi5fjF2p"
   },
   "outputs": [],
   "source": [
    "##train a network using the generated embeddings as features\n",
    "#first load the embeddings from the files into a matrix\n",
    "f = open('Embeddings/words.txt')\n",
    "words = f.readlines()\n",
    "f.close()\n",
    "f = open('Embeddings/vectors.txt')\n",
    "vectors = f.readlines()\n",
    "f.close()\n",
    "\n",
    "embeddings = dict()\n",
    "for i,w in enumerate(words):\n",
    "  embeddings[w.strip()] = np.array([float(x) for x in vectors[i].strip().split('\\t')])\n",
    "embedding_size = len(vectors[i].strip().split('\\t'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "X4wRfkgUjU5Q",
    "outputId": "f91bcf69-6766-400f-8ff9-fa9fa4eedd2c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "914\n",
      "914\n",
      "[-1.82360578  0.34138749  0.29822487 -0.75997798  0.83217859  0.601778\n",
      "  0.99570207  0.13635753  0.61127107  0.02886601  0.83160091  0.517771\n",
      " -0.73988049 -0.01649273  0.25642619 -0.13726492  0.55010723 -1.24789482\n",
      "  1.08451814 -0.2062002  -0.50446282  0.17046533 -0.27019463  0.18842409\n",
      " -0.70414968 -0.6173791   1.05704221 -0.9409136  -0.08095331  0.557665\n",
      "  0.54538365 -0.21563965  0.05484496 -0.27321648 -0.45208534 -0.01071257\n",
      "  1.30831528  0.52625068  0.13732151 -0.21848837  0.51412899  0.54774888\n",
      " -0.29085673 -0.94079209  1.30343695  0.01587903  1.73652809 -0.67173743\n",
      "  0.70545936 -1.6212308 ]\n",
      "[1, 0]\n"
     ]
    }
   ],
   "source": [
    "#for each sentence in the training set, generate sentence embeddings by averaging word embeddings\n",
    "train_X = []\n",
    "train_Y = []\n",
    "for l in train_lines:\n",
    "    s = l.lower().strip().split('\\t')\n",
    "    text = text_cleaner(s[0].strip())\n",
    "    label = int(s[1].strip())\n",
    "    temp_x = np.array([0]*embedding_size)\n",
    "    #represent each sentence by a sentence embedding obtained by averaging word embeddings\n",
    "    for v in text:\n",
    "        if v in embeddings:\n",
    "            temp_x = np.add(temp_x,embeddings[v])\n",
    "        else:\n",
    "            temp_x = np.add(temp_x,np.array([0]*embedding_size))\n",
    "    temp_x = np.true_divide(temp_x,len(text))\n",
    "    \n",
    "    #represent the labels as one-hot vectors\n",
    "    temp_y = [0]*2\n",
    "    temp_y[label] = 1\n",
    "    \n",
    "    train_X.append(temp_x)\n",
    "    train_Y.append(temp_y)\n",
    "    \n",
    "print (len(train_X))\n",
    "print (len(train_Y))\n",
    "print (train_X[1])\n",
    "print (train_Y[1])\n",
    "train_X = np.array(train_X)\n",
    "train_Y = np.array(train_Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "colab_type": "code",
    "id": "UFbHYRyElkOT",
    "outputId": "a438200c-1e5b-49a6-de67-45b51bea0c61"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "229\n",
      "229\n",
      "[-0.7830701  -0.02735568  0.12761055 -0.10032103  0.16476611  0.83002078\n",
      "  0.16052663 -0.1633243   0.62122573  0.42775442  0.20475679  0.6044208\n",
      " -0.14724428 -0.04233229  0.07015692  0.05374405 -0.05922202 -0.13086578\n",
      "  0.12168479  0.22908168 -0.07538843  0.43966649 -0.3635379  -0.09805278\n",
      " -0.3774895  -0.03848179  0.2325461   0.05886424 -0.00449832 -0.2243597\n",
      "  0.69115194 -0.18041525 -0.44948333  0.35550103  0.04634367  0.03371075\n",
      "  0.12027452  0.49152738 -0.14274248  0.01306736 -0.06210457  0.00196507\n",
      "  0.3891153  -0.6353913   0.71120173  0.2722335   0.68436854 -0.44881822\n",
      "  0.56755404 -0.73409353]\n",
      "[1, 0]\n"
     ]
    }
   ],
   "source": [
    "#Similarly represent the test set using word embeddings\n",
    "test_X = []\n",
    "test_Y = []\n",
    "for l in test_lines:\n",
    "    s = l.lower().strip().split('\\t')\n",
    "    text = text_cleaner(s[0].strip())\n",
    "    label = int(s[1].strip())\n",
    "    temp_x = np.array([0]*embedding_size)\n",
    "    #represent each sentence by a sentence embedding obtained by averaging word embeddings\n",
    "    for v in text:\n",
    "        if v in embeddings:\n",
    "            temp_x = np.add(temp_x,embeddings[v])\n",
    "        else:\n",
    "            temp_x = np.add(temp_x,np.array([0]*embedding_size))\n",
    "    temp_x = np.true_divide(temp_x,len(text))\n",
    "    \n",
    "    #represent the labels as one-hot vectors\n",
    "    temp_y = [0]*2\n",
    "    temp_y[label] = 1\n",
    "    \n",
    "    test_X.append(temp_x)\n",
    "    test_Y.append(temp_y)\n",
    "    \n",
    "print (len(test_X))\n",
    "print (len(test_Y))\n",
    "print (test_X[1])\n",
    "print (test_Y[1])\n",
    "test_X = np.array(test_X)\n",
    "test_Y = np.array(test_Y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KG4CJGzMoO_P"
   },
   "source": [
    "# Define a model and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 714
    },
    "colab_type": "code",
    "id": "5s31Uk28mGJh",
    "outputId": "f2c5e3eb-5950-4e92-9ad1-d58f46d00f6e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "914/914 [==============================] - 0s 78us/step - loss: 0.7556 - accuracy: 0.4716\n",
      "Epoch 2/20\n",
      "914/914 [==============================] - 0s 27us/step - loss: 0.6556 - accuracy: 0.6565\n",
      "Epoch 3/20\n",
      "914/914 [==============================] - 0s 25us/step - loss: 0.6300 - accuracy: 0.6882\n",
      "Epoch 4/20\n",
      "914/914 [==============================] - 0s 30us/step - loss: 0.6163 - accuracy: 0.6904\n",
      "Epoch 5/20\n",
      "914/914 [==============================] - 0s 32us/step - loss: 0.6057 - accuracy: 0.6926\n",
      "Epoch 6/20\n",
      "914/914 [==============================] - 0s 33us/step - loss: 0.5971 - accuracy: 0.6969\n",
      "Epoch 7/20\n",
      "914/914 [==============================] - 0s 34us/step - loss: 0.5896 - accuracy: 0.7002\n",
      "Epoch 8/20\n",
      "914/914 [==============================] - 0s 32us/step - loss: 0.5832 - accuracy: 0.7024\n",
      "Epoch 9/20\n",
      "914/914 [==============================] - 0s 32us/step - loss: 0.5773 - accuracy: 0.7079\n",
      "Epoch 10/20\n",
      "914/914 [==============================] - 0s 26us/step - loss: 0.5723 - accuracy: 0.7068\n",
      "Epoch 11/20\n",
      "914/914 [==============================] - 0s 31us/step - loss: 0.5681 - accuracy: 0.7090\n",
      "Epoch 12/20\n",
      "914/914 [==============================] - 0s 33us/step - loss: 0.5644 - accuracy: 0.7166\n",
      "Epoch 13/20\n",
      "914/914 [==============================] - 0s 28us/step - loss: 0.5606 - accuracy: 0.7221\n",
      "Epoch 14/20\n",
      "914/914 [==============================] - 0s 30us/step - loss: 0.5576 - accuracy: 0.7298\n",
      "Epoch 15/20\n",
      "914/914 [==============================] - 0s 34us/step - loss: 0.5547 - accuracy: 0.7298\n",
      "Epoch 16/20\n",
      "914/914 [==============================] - 0s 31us/step - loss: 0.5522 - accuracy: 0.7363\n",
      "Epoch 17/20\n",
      "914/914 [==============================] - 0s 32us/step - loss: 0.5498 - accuracy: 0.7385\n",
      "Epoch 18/20\n",
      "914/914 [==============================] - 0s 31us/step - loss: 0.5477 - accuracy: 0.7374\n",
      "Epoch 19/20\n",
      "914/914 [==============================] - 0s 26us/step - loss: 0.5459 - accuracy: 0.7352\n",
      "Epoch 20/20\n",
      "914/914 [==============================] - 0s 38us/step - loss: 0.5439 - accuracy: 0.7396\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f8ee6232890>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model = Sequential() \n",
    "model.add(Dense(50,input_dim=embedding_size,activation='tanh')) \n",
    "model.add(Dense(2,activation='softmax'))\n",
    "model.compile(optimizer='sgd', loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "model.fit(train_X, train_Y, epochs=20, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "_looteLfmJz8",
    "outputId": "f3840b01-0d36-41aa-85e8-b8cfe029dbaa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "229/229 [==============================] - 0s 132us/step\n"
     ]
    }
   ],
   "source": [
    "res = model.evaluate(test_X,test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4ld2fV2gmf7J"
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "FKZKYbTdmhkn",
    "outputId": "d6921cee-1180-4fe4-cf24-93206262043e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "914\n",
      "914\n",
      "[-1.82360578  0.34138749  0.29822487 -0.75997798  0.83217859  0.601778\n",
      "  0.99570207  0.13635753  0.61127107  0.02886601  0.83160091  0.517771\n",
      " -0.73988049 -0.01649273  0.25642619 -0.13726492  0.55010723 -1.24789482\n",
      "  1.08451814 -0.2062002  -0.50446282  0.17046533 -0.27019463  0.18842409\n",
      " -0.70414968 -0.6173791   1.05704221 -0.9409136  -0.08095331  0.557665\n",
      "  0.54538365 -0.21563965  0.05484496 -0.27321648 -0.45208534 -0.01071257\n",
      "  1.30831528  0.52625068  0.13732151 -0.21848837  0.51412899  0.54774888\n",
      " -0.29085673 -0.94079209  1.30343695  0.01587903  1.73652809 -0.67173743\n",
      "  0.70545936 -1.6212308 ]\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "#for each sentence in the training set, generate sentence embeddings by averaging word embeddings\n",
    "train_X = []\n",
    "train_Y = []\n",
    "for l in train_lines:\n",
    "    s = l.lower().strip().split('\\t')\n",
    "    text = text_cleaner(s[0].strip())\n",
    "    label = int(s[1].strip())\n",
    "    temp_x = np.array([0]*embedding_size)\n",
    "    #represent each sentence by a sentence embedding obtained by averaging word embeddings\n",
    "    for v in text:\n",
    "        if v in embeddings:\n",
    "            temp_x = np.add(temp_x,embeddings[v])\n",
    "        else:\n",
    "            temp_x = np.add(temp_x,np.array([0]*embedding_size))\n",
    "    temp_x = np.true_divide(temp_x,len(text))\n",
    "    \n",
    "    #represent the labels as one-hot vectors\n",
    "    train_X.append(temp_x)\n",
    "    train_Y.append(label)\n",
    "    \n",
    "print (len(train_X))\n",
    "print (len(train_Y))\n",
    "print (train_X[1])\n",
    "print (train_Y[1])\n",
    "train_X = np.array(train_X)\n",
    "train_Y = np.array(train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "colab_type": "code",
    "id": "SLeCFreJnKSj",
    "outputId": "a379a931-f465-4f06-9752-fe44f9410092"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "229\n",
      "229\n",
      "[-0.7830701  -0.02735568  0.12761055 -0.10032103  0.16476611  0.83002078\n",
      "  0.16052663 -0.1633243   0.62122573  0.42775442  0.20475679  0.6044208\n",
      " -0.14724428 -0.04233229  0.07015692  0.05374405 -0.05922202 -0.13086578\n",
      "  0.12168479  0.22908168 -0.07538843  0.43966649 -0.3635379  -0.09805278\n",
      " -0.3774895  -0.03848179  0.2325461   0.05886424 -0.00449832 -0.2243597\n",
      "  0.69115194 -0.18041525 -0.44948333  0.35550103  0.04634367  0.03371075\n",
      "  0.12027452  0.49152738 -0.14274248  0.01306736 -0.06210457  0.00196507\n",
      "  0.3891153  -0.6353913   0.71120173  0.2722335   0.68436854 -0.44881822\n",
      "  0.56755404 -0.73409353]\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "#Similarly represent the test set using word embeddings\n",
    "test_X = []\n",
    "test_Y = []\n",
    "for l in test_lines:\n",
    "    s = l.lower().strip().split('\\t')\n",
    "    text = text_cleaner(s[0].strip())\n",
    "    label = int(s[1].strip())\n",
    "    temp_x = np.array([0]*embedding_size)\n",
    "    #represent each sentence by a sentence embedding obtained by averaging word embeddings\n",
    "    for v in text:\n",
    "        if v in embeddings:\n",
    "            temp_x = np.add(temp_x,embeddings[v])\n",
    "        else:\n",
    "            temp_x = np.add(temp_x,np.array([0]*embedding_size))\n",
    "    temp_x = np.true_divide(temp_x,len(text))\n",
    "    \n",
    "    #represent the labels as one-hot vectors\n",
    "    \n",
    "    \n",
    "    test_X.append(temp_x)\n",
    "    test_Y.append(label)\n",
    "    \n",
    "print (len(test_X))\n",
    "print (len(test_Y))\n",
    "print (test_X[1])\n",
    "print (test_Y[1])\n",
    "test_X = np.array(test_X)\n",
    "test_Y = np.array(test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3qUf4Dp6nk9S"
   },
   "outputs": [],
   "source": [
    "gnb= GaussianNB()\n",
    "model = gnb.fit(train_X,train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jZ0uoguXn1ys"
   },
   "outputs": [],
   "source": [
    "y = model.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f_KB_8MKn8pO"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "jqG0l8lqoBXs",
    "outputId": "77cc9fb8-cf36-4cc4-c0f4-a75776f2450d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy  0.74235807860262\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy \",accuracy_score(test_Y, y, normalize = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1,\n",
       "       1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0,\n",
       "       0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0,\n",
       "       1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n",
       "       0, 0, 1, 0, 1, 1, 0, 1, 0])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JvMhWVWNoQfn"
   },
   "outputs": [],
   "source": [
    "svml=svm.LinearSVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "y5LNbTFuohnm",
    "outputId": "d2918f4b-92db-4095-aeae-f8567c1f6a20"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gyanendro/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "model = svml.fit(train_X,train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x1OfQ8jjonYK"
   },
   "outputs": [],
   "source": [
    "y = model.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "VzMI1ti-oveP",
    "outputId": "6950f58f-5dcd-4667-ba01-b323b2d25621"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy  0.6812227074235808\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy \",accuracy_score(test_Y, y, normalize = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "LabDemo1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
