{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1143\n"
     ]
    }
   ],
   "source": [
    "#read input\n",
    "f=open('Dataset/sentiment.txt','r')\n",
    "lines=f.readlines()\n",
    "f.close()\n",
    "print (len(lines))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "\n",
    "stopwords = [\"a\", \"about\", \"above\", \"above\", \"across\", \"after\", \"afterwards\", \"again\", \"against\", \"all\", \"almost\", \"alone\", \"along\", \"already\", \"also\",\"although\",\"always\",\"am\",\"among\", \"amongst\", \"amoungst\", \"amount\",  \"an\", \"and\", \"another\", \"any\",\"anyhow\",\"anyone\",\"anything\",\"anyway\", \"anywhere\", \"are\", \"around\", \"as\",  \"at\", \"back\",\"be\",\"became\", \"because\",\"become\",\"becomes\", \"becoming\", \"been\", \"before\", \"beforehand\", \"behind\", \"being\", \"below\", \"beside\", \"besides\", \"between\", \"beyond\", \"bill\", \"both\", \"bottom\",\"but\", \"by\", \"call\", \"can\", \"cannot\", \"cant\", \"co\", \"con\", \"could\", \"couldnt\", \"cry\", \"de\", \"describe\", \"detail\", \"do\", \"done\", \"down\", \"due\", \"during\", \"each\", \"eg\", \"eight\", \"either\", \"eleven\",\"else\", \"elsewhere\", \"empty\", \"enough\", \"etc\", \"even\", \"ever\", \"every\", \"everyone\", \"everything\", \"everywhere\", \"except\", \"few\", \"fifteen\", \"fify\", \"fill\", \"find\", \"fire\", \"first\", \"five\", \"for\", \"former\", \"formerly\", \"forty\", \"found\", \"four\", \"from\", \"front\", \"full\", \"further\", \"get\", \"give\", \"go\", \"had\", \"has\", \"hasnt\", \"have\", \"he\", \"hence\", \"her\", \"here\", \"hereafter\", \"hereby\", \"herein\", \"hereupon\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"however\", \"hundred\", \"ie\", \"if\", \"in\", \"inc\", \"indeed\", \"interest\", \"into\", \"is\", \"it\", \"its\", \"itself\", \"keep\", \"last\", \"latter\", \"latterly\", \"least\", \"less\", \"ltd\", \"made\", \"many\", \"may\", \"me\", \"meanwhile\", \"might\", \"mill\", \"mine\", \"more\", \"moreover\", \"most\", \"mostly\", \"move\", \"much\", \"must\", \"my\", \"myself\", \"name\", \"namely\", \"neither\", \"never\", \"nevertheless\", \"next\", \"nine\", \"no\", \"nobody\", \"none\", \"noone\", \"nor\", \"not\", \"nothing\", \"now\", \"nowhere\", \"of\", \"off\", \"often\", \"on\", \"once\", \"one\", \"only\", \"onto\", \"or\", \"other\", \"others\", \"otherwise\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\",\"part\", \"per\", \"perhaps\", \"please\", \"put\", \"rather\", \"re\", \"same\", \"see\", \"seem\", \"seemed\", \"seeming\", \"seems\", \"serious\", \"several\", \"she\", \"should\", \"show\", \"side\", \"since\", \"sincere\", \"six\", \"sixty\", \"so\", \"some\", \"somehow\", \"someone\", \"something\", \"sometime\", \"sometimes\", \"somewhere\", \"still\", \"such\", \"system\", \"take\", \"ten\", \"than\", \"that\", \"the\", \"their\", \"them\", \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\", \"therefore\", \"therein\", \"thereupon\", \"these\", \"they\", \"thickv\", \"thin\", \"third\", \"this\", \"those\", \"though\", \"three\", \"through\", \"throughout\", \"thru\", \"thus\", \"to\", \"together\", \"too\", \"top\", \"toward\", \"towards\", \"twelve\", \"twenty\", \"two\", \"un\", \"under\", \"until\", \"up\", \"upon\", \"us\", \"very\", \"via\", \"was\", \"we\", \"well\", \"were\", \"what\", \"whatever\", \"when\", \"whence\", \"whenever\", \"where\", \"whereafter\", \"whereas\", \"whereby\", \"wherein\", \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\", \"whither\", \"who\", \"whoever\", \"whole\", \"whom\", \"whose\", \"why\", \"will\", \"with\", \"within\", \"without\", \"would\", \"yet\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"the\"]\n",
    "\n",
    "def text_cleaner(text): \n",
    "    text=remove_link(text.lower())\n",
    "    long_words=[]\n",
    "    for i in text.split():\n",
    "        if i not in stopwords:                  \n",
    "            long_words.append(i)\n",
    "    return long_words\n",
    "\n",
    "def remove_link(text):\n",
    "    regex = r'https?://[^\\s<>)\"‘’]+'\n",
    "    match = re.sub(regex,' ', text)\n",
    "    regex = r'https?:|urls?|[/\\:,-.\"\\'?!;…]+'\n",
    "    tweet = re.sub(regex,' ', match)\n",
    "    tweet = re.sub(\"[^a-zA-Z_]\", \" \", tweet)\n",
    "    tweet = re.sub(\"[ ]+\", \" \", tweet) \n",
    "    return tweet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size :  914\n",
      "Test set size :  229\n"
     ]
    }
   ],
   "source": [
    "#divide the data into train set and test set\n",
    "import random\n",
    "random.seed(1)\n",
    "random.shuffle(lines) #shuffle the dataset before dividing it into train and test set\n",
    "split_size = int(0.8*len(lines)) #use 80% of total data as train set and 20% as test set\n",
    "train_lines = lines[:split_size]\n",
    "test_lines = lines[split_size:]\n",
    "\n",
    "print (\"Training set size : \", len(train_lines))\n",
    "print (\"Test set size : \", len(test_lines))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size :  4012\n",
      "Max sentence length :  19\n"
     ]
    }
   ],
   "source": [
    "#convert string tokens to integers\n",
    "#create a vocabulary set and assign a unique id to each word in the vocabulary\n",
    "\n",
    "#load all unique vocabulary\n",
    "vocab = []\n",
    "maxlen = []\n",
    "for l in train_lines:\n",
    "    s = l.strip().split('\\t')\n",
    "    words = text_cleaner(s[0].strip())\n",
    "    vocab += words\n",
    "    maxlen.append(len(words))\n",
    "vocab = list(set(vocab))\n",
    "print (\"Vocabulary size : \", len(vocab))\n",
    "\n",
    "#assign unique id to each vocabulary\n",
    "word2id = dict()\n",
    "for i,v in enumerate(vocab,1):\n",
    "    word2id[v] = i\n",
    "word2id['PAD'] = 0 #special token to take care of unseen words in the test set\n",
    "maxlen = max(maxlen)\n",
    "print (\"Max sentence length : \",maxlen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "914\n",
      "914\n",
      "[1618, 190, 1232, 3502, 254, 1111, 1111, 776, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 0]\n"
     ]
    }
   ],
   "source": [
    "#Prepare train and test set\n",
    "#Convert strings to integers\n",
    "#prepare train and test set\n",
    "\n",
    "#prepare train set\n",
    "import numpy as np\n",
    "train_X = []\n",
    "train_Y = []\n",
    "for l in train_lines:\n",
    "    s = l.strip().split('\\t')\n",
    "    text = text_cleaner(s[0])\n",
    "    label = int(s[1].strip())\n",
    "    temp_x = [word2id[x] for x in text]\n",
    "    temp_x += [0]* (maxlen-len(temp_x)) #convert all input to equal size to enable training in batches\n",
    "    temp_y = [0]*2\n",
    "    temp_y[label] = 1\n",
    "    train_X.append(temp_x)\n",
    "    train_Y.append(temp_y)\n",
    "print (len(train_X))\n",
    "print (len(train_Y))\n",
    "print (train_X[0])\n",
    "print (train_Y[0])\n",
    "train_X = np.array(train_X)\n",
    "train_Y = np.array(train_Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(914, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "229\n",
      "229\n",
      "[3821, 878, 3871, 1615, 0, 0, 3940, 944, 890, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 0]\n"
     ]
    }
   ],
   "source": [
    "#prepare test set\n",
    "test_X = []\n",
    "test_Y = []\n",
    "\n",
    "for l in test_lines:\n",
    "    s = l.strip().split('\\t')\n",
    "    text = text_cleaner(s[0].strip())\n",
    "    label = int(s[1].strip())\n",
    "    temp_x = [word2id[x] if x in word2id else 0 for x in text ][:maxlen]\n",
    "    temp_x += [0]* (maxlen-len(temp_x)) #convert all input to equal size to enable training in batches\n",
    "    temp_y = [0]*2\n",
    "    temp_y[label] = 1\n",
    "    test_X.append(temp_x)\n",
    "    test_Y.append(temp_y)\n",
    "print (len(test_X))\n",
    "print (len(test_Y))\n",
    "print (test_X[0])\n",
    "print (test_Y[0])\n",
    "test_X = np.array(test_X)\n",
    "test_Y = np.array(test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential #Sequential is the class in Keras library that defines a model comprising of linear stack of models\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Dropout, Embedding\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers import LSTM, Bidirectional,SimpleRNN\n",
    "\n",
    "input_tweet=len(train_X[0])\n",
    "input_node=Input(shape=(input_tweet,))\n",
    "emb=60\n",
    "lstmnode=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gyanendro/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "914/914 [==============================] - 0s 242us/step - loss: 0.6644 - accuracy: 0.6904\n",
      "Epoch 2/10\n",
      "914/914 [==============================] - 0s 105us/step - loss: 0.6390 - accuracy: 0.6904\n",
      "Epoch 3/10\n",
      "914/914 [==============================] - 0s 103us/step - loss: 0.6168 - accuracy: 0.6904\n",
      "Epoch 4/10\n",
      "914/914 [==============================] - 0s 109us/step - loss: 0.5992 - accuracy: 0.6904\n",
      "Epoch 5/10\n",
      "914/914 [==============================] - 0s 115us/step - loss: 0.5846 - accuracy: 0.6904\n",
      "Epoch 6/10\n",
      "914/914 [==============================] - 0s 115us/step - loss: 0.5724 - accuracy: 0.6904\n",
      "Epoch 7/10\n",
      "914/914 [==============================] - 0s 109us/step - loss: 0.5605 - accuracy: 0.6904\n",
      "Epoch 8/10\n",
      "914/914 [==============================] - 0s 112us/step - loss: 0.5494 - accuracy: 0.6904\n",
      "Epoch 9/10\n",
      "914/914 [==============================] - 0s 113us/step - loss: 0.5372 - accuracy: 0.6904\n",
      "Epoch 10/10\n",
      "914/914 [==============================] - 0s 108us/step - loss: 0.5230 - accuracy: 0.6904\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fdb82502f90>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def CNN(input_node,max_len):\n",
    "    encode=Embedding(len(vocab)+2, emb, input_length=max_len, trainable=True)(input_node)    #,weights=[vectors],trainable=False\n",
    "    encode=Conv1D(128,\n",
    "                         3,\n",
    "                         padding='same',\n",
    "                         activation='relu',\n",
    "                         strides=1)(encode)\n",
    "    encode=GlobalMaxPooling1D()(encode)\n",
    "    return encode\n",
    "\n",
    "cnn_out=CNN(input_node,input_tweet)\n",
    "senti_class=Dense(2,activation='softmax')(cnn_out)\n",
    "model=Model(input_node,senti_class)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(train_X, train_Y,\n",
    "          batch_size=512,\n",
    "          epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "229/229 [==============================] - 0s 182us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6339120339097934, 0.6462882161140442]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = model.evaluate(test_X,test_Y)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We blame cities for the majority of CO2 emissions without acknowledging their vulnerability to #CFCC15 #journey2015 #S2228 #SemST\t0 [0.7098524  0.29014763]\n",
      "Feminists who go for a gender studies degree should also blame the patriarchy for their mediocre grades in science. #SemST\t0 [0.7279925  0.27200747]\n",
      "Just wrote my blog to help @CalAlimony pass a vital law that ends #alimony. Posting soon. #Divorce #leanin #SemST\t1 [0.7111126  0.28888738]\n",
      "RT @JohnFugelsang: They should just make the GOP primaries a reality game show called \"Who Wants To Get Beat Up By A Girl? #SemST\t0 [0.72295177 0.2770483 ]\n",
      "It's incredibly easy to identify shitty females with a poor view on the world and what's important thanks to #SemST\t0 [0.71841705 0.28158298]\n"
     ]
    }
   ],
   "source": [
    "p = model.predict(test_X)\n",
    "for i,l in enumerate(test_lines[:5]):\n",
    "  print (l.strip(),p[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gyanendro/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "914/914 [==============================] - 0s 412us/step - loss: 0.6879 - accuracy: 0.5416\n",
      "Epoch 2/10\n",
      "914/914 [==============================] - 0s 77us/step - loss: 0.5874 - accuracy: 0.6958\n",
      "Epoch 3/10\n",
      "914/914 [==============================] - 0s 74us/step - loss: 0.5458 - accuracy: 0.6937\n",
      "Epoch 4/10\n",
      "914/914 [==============================] - 0s 82us/step - loss: 0.4803 - accuracy: 0.7068\n",
      "Epoch 5/10\n",
      "914/914 [==============================] - 0s 77us/step - loss: 0.3861 - accuracy: 0.8074\n",
      "Epoch 6/10\n",
      "914/914 [==============================] - 0s 71us/step - loss: 0.2745 - accuracy: 0.9530\n",
      "Epoch 7/10\n",
      "914/914 [==============================] - 0s 83us/step - loss: 0.1631 - accuracy: 0.9858\n",
      "Epoch 8/10\n",
      "914/914 [==============================] - 0s 72us/step - loss: 0.0811 - accuracy: 0.9880\n",
      "Epoch 9/10\n",
      "914/914 [==============================] - 0s 71us/step - loss: 0.0388 - accuracy: 0.9923\n",
      "Epoch 10/10\n",
      "914/914 [==============================] - 0s 110us/step - loss: 0.0201 - accuracy: 0.9967\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fdb707d7190>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def RNN(input_node,max_len):\n",
    "    encode=Embedding(len(vocab)+2, emb, input_length=max_len, trainable=True)(input_node)   \n",
    "    encoder_RNN=SimpleRNN(lstmnode,activation='tanh')(encode)\n",
    "    return encoder_RNN\n",
    "\n",
    "rnn_out=RNN(input_node,input_tweet)\n",
    "senti_class=Dense(2,activation='softmax')(rnn_out)\n",
    "model=Model(input_node,senti_class)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(train_X, train_Y,\n",
    "          batch_size=512,\n",
    "          epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gyanendro/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "914/914 [==============================] - 1s 743us/step - loss: 0.6934 - accuracy: 0.4737\n",
      "Epoch 2/10\n",
      "914/914 [==============================] - 0s 190us/step - loss: 0.6729 - accuracy: 0.6904\n",
      "Epoch 3/10\n",
      "914/914 [==============================] - 0s 212us/step - loss: 0.6555 - accuracy: 0.6904\n",
      "Epoch 4/10\n",
      "914/914 [==============================] - 0s 208us/step - loss: 0.6396 - accuracy: 0.6904\n",
      "Epoch 5/10\n",
      "914/914 [==============================] - 0s 202us/step - loss: 0.6266 - accuracy: 0.6904\n",
      "Epoch 6/10\n",
      "914/914 [==============================] - 0s 214us/step - loss: 0.6172 - accuracy: 0.6904\n",
      "Epoch 7/10\n",
      "914/914 [==============================] - 0s 207us/step - loss: 0.6163 - accuracy: 0.6904\n",
      "Epoch 8/10\n",
      "914/914 [==============================] - 0s 210us/step - loss: 0.6149 - accuracy: 0.6904\n",
      "Epoch 9/10\n",
      "914/914 [==============================] - 0s 203us/step - loss: 0.6069 - accuracy: 0.6904\n",
      "Epoch 10/10\n",
      "914/914 [==============================] - 0s 206us/step - loss: 0.5960 - accuracy: 0.6904\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fdb701df090>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f_LSTM(input_node,max_len):\n",
    "    encode=Embedding(len(vocab)+2, emb, input_length=max_len, trainable=True)(input_node)   \n",
    "    encoder_LSTM=LSTM(lstmnode,activation='tanh')(encode)\n",
    "    return encoder_LSTM\n",
    "\n",
    "LSTM_out=f_LSTM(input_node,input_tweet)\n",
    "senti_class=Dense(2,activation='softmax')(LSTM_out)\n",
    "model=Model(input_node,senti_class)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(train_X, train_Y,\n",
    "          batch_size=512,\n",
    "          epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "229/229 [==============================] - 0s 532us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6610513730340649, 0.6462882161140442]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = model.evaluate(test_X,test_Y)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We blame cities for the majority of CO2 emissions without acknowledging their vulnerability to #CFCC15 #journey2015 #S2228 #SemST\t0 [0.7246813  0.27531865]\n",
      "Feminists who go for a gender studies degree should also blame the patriarchy for their mediocre grades in science. #SemST\t0 [0.7384692 0.2615308]\n",
      "Just wrote my blog to help @CalAlimony pass a vital law that ends #alimony. Posting soon. #Divorce #leanin #SemST\t1 [0.7358404 0.2641597]\n",
      "RT @JohnFugelsang: They should just make the GOP primaries a reality game show called \"Who Wants To Get Beat Up By A Girl? #SemST\t0 [0.7208876  0.27911237]\n",
      "It's incredibly easy to identify shitty females with a poor view on the world and what's important thanks to #SemST\t0 [0.7100063  0.28999373]\n"
     ]
    }
   ],
   "source": [
    "p = model.predict(test_X)\n",
    "for i,l in enumerate(test_lines[:5]):\n",
    "  print (l.strip(),p[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gyanendro/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "914/914 [==============================] - 1s 1ms/step - loss: 0.6908 - accuracy: 0.5088\n",
      "Epoch 2/10\n",
      "914/914 [==============================] - 0s 345us/step - loss: 0.6721 - accuracy: 0.6904\n",
      "Epoch 3/10\n",
      "914/914 [==============================] - 0s 358us/step - loss: 0.6526 - accuracy: 0.6904\n",
      "Epoch 4/10\n",
      "914/914 [==============================] - 0s 344us/step - loss: 0.6327 - accuracy: 0.6904\n",
      "Epoch 5/10\n",
      "914/914 [==============================] - 0s 345us/step - loss: 0.6138 - accuracy: 0.6904\n",
      "Epoch 6/10\n",
      "914/914 [==============================] - 0s 348us/step - loss: 0.6028 - accuracy: 0.6904\n",
      "Epoch 7/10\n",
      "914/914 [==============================] - 0s 346us/step - loss: 0.5999 - accuracy: 0.6904\n",
      "Epoch 8/10\n",
      "914/914 [==============================] - 0s 345us/step - loss: 0.5880 - accuracy: 0.6904\n",
      "Epoch 9/10\n",
      "914/914 [==============================] - 0s 362us/step - loss: 0.5694 - accuracy: 0.6904\n",
      "Epoch 10/10\n",
      "914/914 [==============================] - 0s 346us/step - loss: 0.5498 - accuracy: 0.6904\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fdb584fbd90>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def BiLSTM(input_node,max_len):\n",
    "    encode=Embedding(len(vocab)+2, emb, input_length=max_len, trainable=True)(input_node)   \n",
    "    encoder_BiLSTM=Bidirectional(LSTM(lstmnode,activation='tanh'))(encode)\n",
    "    return encoder_BiLSTM\n",
    "\n",
    "BiLSTM_out=BiLSTM(input_node,input_tweet)\n",
    "senti_class=Dense(2,activation='softmax')(BiLSTM_out)\n",
    "model=Model(input_node,senti_class)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(train_X, train_Y,\n",
    "          batch_size=512,\n",
    "          epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "229/229 [==============================] - 0s 813us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6337413311525203, 0.6462882161140442]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = model.evaluate(test_X,test_Y)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We blame cities for the majority of CO2 emissions without acknowledging their vulnerability to #CFCC15 #journey2015 #S2228 #SemST\t0 [0.71733654 0.28266352]\n",
      "Feminists who go for a gender studies degree should also blame the patriarchy for their mediocre grades in science. #SemST\t0 [0.7383245  0.26167548]\n",
      "Just wrote my blog to help @CalAlimony pass a vital law that ends #alimony. Posting soon. #Divorce #leanin #SemST\t1 [0.6997612 0.3002388]\n",
      "RT @JohnFugelsang: They should just make the GOP primaries a reality game show called \"Who Wants To Get Beat Up By A Girl? #SemST\t0 [0.7068976  0.29310232]\n",
      "It's incredibly easy to identify shitty females with a poor view on the world and what's important thanks to #SemST\t0 [0.70399135 0.2960087 ]\n"
     ]
    }
   ],
   "source": [
    "p = model.predict(test_X)\n",
    "for i,l in enumerate(test_lines[:5]):\n",
    "  print (l.strip(),p[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
