{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IjcqIMAtfWjX"
   },
   "source": [
    "## **Part I: Building deeplearning models using Keras toolkit**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SqMjxeZMcnGS"
   },
   "source": [
    "**PreProcessing Modules**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "JDDuKWGEQNIK",
    "outputId": "14f84853-e70e-42c4-a2f9-a6053c521a66"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1143\n"
     ]
    }
   ],
   "source": [
    "#read input\n",
    "f=open('Dataset/sentiment.txt','r')\n",
    "lines=f.readlines()\n",
    "f.close()\n",
    "print (len(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T8ByrRLBQe1x"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "\n",
    "stopwords = [\"a\", \"about\", \"above\", \"above\", \"across\", \"after\", \"afterwards\", \"again\", \"against\", \"all\", \"almost\", \"alone\", \"along\", \"already\", \"also\",\"although\",\"always\",\"am\",\"among\", \"amongst\", \"amoungst\", \"amount\",  \"an\", \"and\", \"another\", \"any\",\"anyhow\",\"anyone\",\"anything\",\"anyway\", \"anywhere\", \"are\", \"around\", \"as\",  \"at\", \"back\",\"be\",\"became\", \"because\",\"become\",\"becomes\", \"becoming\", \"been\", \"before\", \"beforehand\", \"behind\", \"being\", \"below\", \"beside\", \"besides\", \"between\", \"beyond\", \"bill\", \"both\", \"bottom\",\"but\", \"by\", \"call\", \"can\", \"cannot\", \"cant\", \"co\", \"con\", \"could\", \"couldnt\", \"cry\", \"de\", \"describe\", \"detail\", \"do\", \"done\", \"down\", \"due\", \"during\", \"each\", \"eg\", \"eight\", \"either\", \"eleven\",\"else\", \"elsewhere\", \"empty\", \"enough\", \"etc\", \"even\", \"ever\", \"every\", \"everyone\", \"everything\", \"everywhere\", \"except\", \"few\", \"fifteen\", \"fify\", \"fill\", \"find\", \"fire\", \"first\", \"five\", \"for\", \"former\", \"formerly\", \"forty\", \"found\", \"four\", \"from\", \"front\", \"full\", \"further\", \"get\", \"give\", \"go\", \"had\", \"has\", \"hasnt\", \"have\", \"he\", \"hence\", \"her\", \"here\", \"hereafter\", \"hereby\", \"herein\", \"hereupon\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"however\", \"hundred\", \"ie\", \"if\", \"in\", \"inc\", \"indeed\", \"interest\", \"into\", \"is\", \"it\", \"its\", \"itself\", \"keep\", \"last\", \"latter\", \"latterly\", \"least\", \"less\", \"ltd\", \"made\", \"many\", \"may\", \"me\", \"meanwhile\", \"might\", \"mill\", \"mine\", \"more\", \"moreover\", \"most\", \"mostly\", \"move\", \"much\", \"must\", \"my\", \"myself\", \"name\", \"namely\", \"neither\", \"never\", \"nevertheless\", \"next\", \"nine\", \"no\", \"nobody\", \"none\", \"noone\", \"nor\", \"not\", \"nothing\", \"now\", \"nowhere\", \"of\", \"off\", \"often\", \"on\", \"once\", \"one\", \"only\", \"onto\", \"or\", \"other\", \"others\", \"otherwise\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\",\"part\", \"per\", \"perhaps\", \"please\", \"put\", \"rather\", \"re\", \"same\", \"see\", \"seem\", \"seemed\", \"seeming\", \"seems\", \"serious\", \"several\", \"she\", \"should\", \"show\", \"side\", \"since\", \"sincere\", \"six\", \"sixty\", \"so\", \"some\", \"somehow\", \"someone\", \"something\", \"sometime\", \"sometimes\", \"somewhere\", \"still\", \"such\", \"system\", \"take\", \"ten\", \"than\", \"that\", \"the\", \"their\", \"them\", \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\", \"therefore\", \"therein\", \"thereupon\", \"these\", \"they\", \"thickv\", \"thin\", \"third\", \"this\", \"those\", \"though\", \"three\", \"through\", \"throughout\", \"thru\", \"thus\", \"to\", \"together\", \"too\", \"top\", \"toward\", \"towards\", \"twelve\", \"twenty\", \"two\", \"un\", \"under\", \"until\", \"up\", \"upon\", \"us\", \"very\", \"via\", \"was\", \"we\", \"well\", \"were\", \"what\", \"whatever\", \"when\", \"whence\", \"whenever\", \"where\", \"whereafter\", \"whereas\", \"whereby\", \"wherein\", \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\", \"whither\", \"who\", \"whoever\", \"whole\", \"whom\", \"whose\", \"why\", \"will\", \"with\", \"within\", \"without\", \"would\", \"yet\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"the\"]\n",
    "\n",
    "def text_cleaner(text): \n",
    "    text=remove_link(text.lower())\n",
    "    long_words=[]\n",
    "    for i in text.split():\n",
    "        if i not in stopwords:                  \n",
    "            long_words.append(i)\n",
    "    return long_words\n",
    "\n",
    "def remove_link(text):\n",
    "    regex = r'https?://[^\\s<>)\"‘’]+'\n",
    "    match = re.sub(regex,' ', text)\n",
    "    regex = r'https?:|urls?|[/\\:,-.\"\\'?!;…]+'\n",
    "    tweet = re.sub(regex,' ', match)\n",
    "    tweet = re.sub(\"[^a-zA-Z_]\", \" \", tweet)\n",
    "    tweet = re.sub(\"[ ]+\", \" \", tweet) \n",
    "    return tweet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "q6ccQEucQjeQ",
    "outputId": "9e27536b-7274-4bd3-c9a5-d464a1c445c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size :  914\n",
      "Test set size :  229\n"
     ]
    }
   ],
   "source": [
    "#divide the data into train set and test set\n",
    "import random\n",
    "random.seed(1)\n",
    "random.shuffle(lines) #shuffle the dataset before dividing it into train and test set\n",
    "split_size = int(0.8*len(lines)) #use 80% of total data as train set and 20% as test set\n",
    "train_lines = lines[:split_size]\n",
    "test_lines = lines[split_size:]\n",
    "\n",
    "print (\"Training set size : \", len(train_lines))\n",
    "print (\"Test set size : \", len(test_lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "nnCu4BUFQncO",
    "outputId": "099100b5-e6ff-45b3-ac6c-3cac15c65014"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size :  4012\n",
      "Max sentence length :  19\n"
     ]
    }
   ],
   "source": [
    "#convert string tokens to integers\n",
    "#create a vocabulary set and assign a unique id to each word in the vocabulary\n",
    "\n",
    "#load all unique vocabulary\n",
    "vocab = []\n",
    "maxlen = []\n",
    "for l in train_lines:\n",
    "    s = l.strip().split('\\t')\n",
    "    words = text_cleaner(s[0].strip())\n",
    "    vocab += words\n",
    "    maxlen.append(len(words))\n",
    "vocab = list(set(vocab))\n",
    "print (\"Vocabulary size : \", len(vocab))\n",
    "\n",
    "#assign unique id to each vocabulary\n",
    "word2id = dict()\n",
    "id2word = dict()\n",
    "for i,v in enumerate(vocab,1):\n",
    "    word2id[v] = i\n",
    "    id2word[i] = v \n",
    "word2id['PAD'] = 0 #special token to take care of unseen words in the test set\n",
    "id2word[0] = 'PAD'\n",
    "maxlen = max(maxlen)\n",
    "print (\"Max sentence length : \",maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "mdGqSrgzQrOX",
    "outputId": "ae9542b7-0507-4d26-883a-ebb6a3ab8aba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "914\n",
      "914\n",
      "[1924, 467, 506, 337, 1081, 2333, 2333, 1676, 464, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[1, 0], [1, 0], [1, 0], [1, 0], [1, 0]]\n"
     ]
    }
   ],
   "source": [
    "#Prepare train and test set\n",
    "#Convert strings to integers\n",
    "#prepare train and test set\n",
    "\n",
    "#prepare train set\n",
    "import numpy as np\n",
    "train_X = []\n",
    "train_Y = []\n",
    "for l in train_lines:\n",
    "    s = l.strip().split('\\t')\n",
    "    text = text_cleaner(s[0])\n",
    "    label = int(s[1].strip())\n",
    "    temp_x = [word2id[x] for x in text]\n",
    "    temp_x += [0]* (maxlen-len(temp_x)) #convert all input to equal size to enable training in batches\n",
    "    temp_y = [0]*2\n",
    "    temp_y[label] = 1\n",
    "    train_X.append(temp_x)\n",
    "    train_Y.append(temp_y)\n",
    "print (len(train_X))\n",
    "print (len(train_Y))\n",
    "print (train_X[0])\n",
    "print (train_Y[:5])\n",
    "train_X = np.array(train_X)\n",
    "train_Y = np.array(train_Y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "GBYAWNicQ0rP",
    "outputId": "62cf0b85-d9d8-491d-ad0f-345603a8b0c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "229\n",
      "229\n",
      "[2011, 3636, 3333, 657, 0, 0, 654, 81, 1121, 464, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 0]\n"
     ]
    }
   ],
   "source": [
    "test_X = []\n",
    "test_Y = []\n",
    "\n",
    "for l in test_lines:\n",
    "    s = l.strip().split('\\t')\n",
    "    text = text_cleaner(s[0].strip())\n",
    "    label = int(s[1].strip())\n",
    "    temp_x = [word2id[x] if x in word2id else 0 for x in text ][:maxlen]\n",
    "    temp_x += [0]* (maxlen-len(temp_x)) #convert all input to equal size to enable training in batches\n",
    "    temp_y = [0]*2\n",
    "    temp_y[label] = 1\n",
    "    test_X.append(temp_x)\n",
    "    test_Y.append(temp_y)\n",
    "print (len(test_X))\n",
    "print (len(test_Y))\n",
    "print (test_X[0])\n",
    "print (test_Y[0])\n",
    "test_X = np.array(test_X)\n",
    "test_Y = np.array(test_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xrrKpNrrfLm_"
   },
   "source": [
    "## Part I: Building deeplearning models using Keras toolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0eEytHRJRB1i"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Dropout, Embedding\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers import LSTM, Bidirectional,SimpleRNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BTwbqaaIcyyt"
   },
   "source": [
    "Text Classification Using CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "id": "Cec9LTpURMeM",
    "outputId": "4b7b1638-065b-4b27-8cdc-7fd4bf885d03"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gyanendro/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "914/914 [==============================] - 2s 2ms/step - loss: 0.6822 - accuracy: 0.6674\n",
      "Epoch 2/10\n",
      "914/914 [==============================] - 0s 93us/step - loss: 0.6565 - accuracy: 0.6904\n",
      "Epoch 3/10\n",
      "914/914 [==============================] - 0s 90us/step - loss: 0.6339 - accuracy: 0.6904\n",
      "Epoch 4/10\n",
      "914/914 [==============================] - 0s 113us/step - loss: 0.6138 - accuracy: 0.6904\n",
      "Epoch 5/10\n",
      "914/914 [==============================] - 0s 117us/step - loss: 0.5962 - accuracy: 0.6904\n",
      "Epoch 6/10\n",
      "914/914 [==============================] - 0s 101us/step - loss: 0.5804 - accuracy: 0.6904\n",
      "Epoch 7/10\n",
      "914/914 [==============================] - 0s 105us/step - loss: 0.5663 - accuracy: 0.6904\n",
      "Epoch 8/10\n",
      "914/914 [==============================] - 0s 109us/step - loss: 0.5522 - accuracy: 0.6904\n",
      "Epoch 9/10\n",
      "914/914 [==============================] - 0s 101us/step - loss: 0.5390 - accuracy: 0.6904\n",
      "Epoch 10/10\n",
      "914/914 [==============================] - 0s 102us/step - loss: 0.5251 - accuracy: 0.6904\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fc8244475d0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_size = 50\n",
    "input=Input(shape=(maxlen,))\n",
    "embedding = Embedding(len(word2id), embedding_size, input_length=maxlen, trainable=True)(input)\n",
    "convolution = Conv1D(128,3,padding='same',activation='relu',strides=1)(embedding)\n",
    "pooling = GlobalMaxPooling1D()(convolution)\n",
    "output = Dense(2,activation='softmax')(pooling)\n",
    "model=Model(input,output)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(train_X, train_Y,batch_size=512,epochs=10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "FE_fsddnSaoE",
    "outputId": "fd992b4a-49e7-41ca-b48a-69db18380ac7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "229/229 [==============================] - 0s 509us/step\n"
     ]
    }
   ],
   "source": [
    "res = model.evaluate(test_X,test_Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rzg8DzDDTu8r"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "rjr-s5M1SeTw",
    "outputId": "146dcc8a-ccb7-434b-910b-a3981cce922d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We blame cities for the majority of CO2 emissions without acknowledging their vulnerability to #CFCC15 #journey2015 #S2228 #SemST\t0 [0.70769495 0.29230514]\n",
      "Feminists who go for a gender studies degree should also blame the patriarchy for their mediocre grades in science. #SemST\t0 [0.7182329 0.2817671]\n",
      "Just wrote my blog to help @CalAlimony pass a vital law that ends #alimony. Posting soon. #Divorce #leanin #SemST\t1 [0.68872374 0.31127623]\n",
      "RT @JohnFugelsang: They should just make the GOP primaries a reality game show called \"Who Wants To Get Beat Up By A Girl? #SemST\t0 [0.71024686 0.28975317]\n",
      "It's incredibly easy to identify shitty females with a poor view on the world and what's important thanks to #SemST\t0 [0.7099096  0.29009038]\n"
     ]
    }
   ],
   "source": [
    "p = model.predict(test_X)\n",
    "for i,l in enumerate(test_lines[:5]):\n",
    "  print (l.strip(),p[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z5zlCHKNc7RR"
   },
   "source": [
    "Loading Pre-trained Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bXtMm-EkSgpE"
   },
   "outputs": [],
   "source": [
    "f = open('Embeddings/words.txt')\n",
    "words = f.readlines()\n",
    "f.close()\n",
    "f = open('Embeddings/vectors.txt')\n",
    "vectors = f.readlines()\n",
    "f.close()\n",
    "\n",
    "embeddings = dict()\n",
    "for i,w in enumerate(words):\n",
    "  embeddings[w.strip()] = np.array([float(x) for x in vectors[i].strip().split('\\t')])\n",
    "embedding_size = len(vectors[i].strip().split('\\t'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qQJHlqOfS0g8"
   },
   "outputs": [],
   "source": [
    "embeddings_matrix = []\n",
    "for i in range(len(word2id)):\n",
    "  if id2word[i] in embeddings:\n",
    "    embeddings_matrix.append(embeddings[id2word[i]])\n",
    "  else:\n",
    "    embeddings_matrix.append(np.array([0.0]*embedding_size))\n",
    "embeddings_matrix = np.array(embeddings_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9SdUyf7CdBPY"
   },
   "source": [
    "Text Classification using CNN and pre-trained embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "id": "RIS2BTrWTfSV",
    "outputId": "65f2167a-ff7c-46d8-dc48-03ab1cd023b1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gyanendro/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "914/914 [==============================] - 0s 212us/step - loss: 0.7365 - accuracy: 0.5274\n",
      "Epoch 2/10\n",
      "914/914 [==============================] - 0s 117us/step - loss: 0.6406 - accuracy: 0.6926\n",
      "Epoch 3/10\n",
      "914/914 [==============================] - 0s 101us/step - loss: 0.6021 - accuracy: 0.7221\n",
      "Epoch 4/10\n",
      "914/914 [==============================] - 0s 104us/step - loss: 0.5403 - accuracy: 0.7429\n",
      "Epoch 5/10\n",
      "914/914 [==============================] - 0s 120us/step - loss: 0.5307 - accuracy: 0.7495\n",
      "Epoch 6/10\n",
      "914/914 [==============================] - 0s 128us/step - loss: 0.5307 - accuracy: 0.7495\n",
      "Epoch 7/10\n",
      "914/914 [==============================] - 0s 125us/step - loss: 0.5036 - accuracy: 0.7604\n",
      "Epoch 8/10\n",
      "914/914 [==============================] - 0s 97us/step - loss: 0.4812 - accuracy: 0.7779\n",
      "Epoch 9/10\n",
      "914/914 [==============================] - 0s 99us/step - loss: 0.4759 - accuracy: 0.7681\n",
      "Epoch 10/10\n",
      "914/914 [==============================] - 0s 111us/step - loss: 0.4658 - accuracy: 0.7713\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fc81c6d7850>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_size = 50\n",
    "input=Input(shape=(maxlen,))\n",
    "embedding = Embedding(len(word2id), embedding_size, input_length=maxlen, weights = [embeddings_matrix],trainable=True)(input)\n",
    "convolution = Conv1D(128,3,padding='same',activation='relu',strides=1)(embedding)\n",
    "pooling = GlobalMaxPooling1D()(convolution)\n",
    "output = Dense(2,activation='softmax')(pooling)\n",
    "model=Model(input,output)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(train_X, train_Y,batch_size=512,epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "pKOkmBOzTwUe",
    "outputId": "697661cb-eb55-470a-9943-d6ed72e0541f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "229/229 [==============================] - 0s 189us/step\n"
     ]
    }
   ],
   "source": [
    "res = model.evaluate(test_X,test_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Uc-SUPC5dIlG"
   },
   "source": [
    "Text Classification using Vanilla RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "id": "tWYKchr4T0yV",
    "outputId": "30e714eb-27f3-4f22-e4a6-da1cdcfce926"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gyanendro/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "914/914 [==============================] - 1s 899us/step - loss: 0.6646 - accuracy: 0.6039\n",
      "Epoch 2/10\n",
      "914/914 [==============================] - 0s 90us/step - loss: 0.6175 - accuracy: 0.6838\n",
      "Epoch 3/10\n",
      "914/914 [==============================] - 0s 71us/step - loss: 0.6068 - accuracy: 0.6915\n",
      "Epoch 4/10\n",
      "914/914 [==============================] - 0s 86us/step - loss: 0.5863 - accuracy: 0.6991\n",
      "Epoch 5/10\n",
      "914/914 [==============================] - 0s 69us/step - loss: 0.5806 - accuracy: 0.7112\n",
      "Epoch 6/10\n",
      "914/914 [==============================] - 0s 72us/step - loss: 0.5665 - accuracy: 0.7199\n",
      "Epoch 7/10\n",
      "914/914 [==============================] - 0s 82us/step - loss: 0.5470 - accuracy: 0.7254\n",
      "Epoch 8/10\n",
      "914/914 [==============================] - 0s 70us/step - loss: 0.5310 - accuracy: 0.7363\n",
      "Epoch 9/10\n",
      "914/914 [==============================] - 0s 83us/step - loss: 0.5101 - accuracy: 0.7538\n",
      "Epoch 10/10\n",
      "914/914 [==============================] - 0s 78us/step - loss: 0.4876 - accuracy: 0.7790\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fc8242b91d0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_size = 50\n",
    "input=Input(shape=(maxlen,))\n",
    "embedding = Embedding(len(word2id), embedding_size, input_length=maxlen, weights = [embeddings_matrix],trainable=True)(input)\n",
    "rnn = SimpleRNN(64,activation='tanh')(embedding)\n",
    "output = Dense(2,activation='softmax')(rnn)\n",
    "model=Model(input,output)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(train_X, train_Y,batch_size=512,epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "yG2Yw6goUQtm",
    "outputId": "b601fa57-2034-4818-c97c-f75bd5c3a284"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "229/229 [==============================] - 0s 562us/step\n"
     ]
    }
   ],
   "source": [
    "res = model.evaluate(test_X,test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zjyeplg9UgeD"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MiInBEzkdOEx"
   },
   "source": [
    "Text Classification Using LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "id": "w9ILCoYVUSgp",
    "outputId": "06b1262d-89d5-4511-fe3f-bd842ae4a74e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gyanendro/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "914/914 [==============================] - 1s 955us/step - loss: 0.6802 - accuracy: 0.6171\n",
      "Epoch 2/10\n",
      "914/914 [==============================] - 0s 190us/step - loss: 0.6360 - accuracy: 0.6904\n",
      "Epoch 3/10\n",
      "914/914 [==============================] - 0s 203us/step - loss: 0.6061 - accuracy: 0.6904\n",
      "Epoch 4/10\n",
      "914/914 [==============================] - 0s 201us/step - loss: 0.5947 - accuracy: 0.6904\n",
      "Epoch 5/10\n",
      "914/914 [==============================] - 0s 203us/step - loss: 0.5955 - accuracy: 0.6904\n",
      "Epoch 6/10\n",
      "914/914 [==============================] - 0s 212us/step - loss: 0.5882 - accuracy: 0.6904\n",
      "Epoch 7/10\n",
      "914/914 [==============================] - 0s 198us/step - loss: 0.5735 - accuracy: 0.6915\n",
      "Epoch 8/10\n",
      "914/914 [==============================] - 0s 196us/step - loss: 0.5619 - accuracy: 0.6980\n",
      "Epoch 9/10\n",
      "914/914 [==============================] - 0s 202us/step - loss: 0.5531 - accuracy: 0.7123\n",
      "Epoch 10/10\n",
      "914/914 [==============================] - 0s 201us/step - loss: 0.5422 - accuracy: 0.7330\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fc7fc473a10>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_size = 50\n",
    "input=Input(shape=(maxlen,))\n",
    "embedding = Embedding(len(word2id), embedding_size, input_length=maxlen, weights = [embeddings_matrix],trainable=True)(input)\n",
    "rnn = LSTM(64,activation='tanh')(embedding)\n",
    "output = Dense(2,activation='softmax')(rnn)\n",
    "model=Model(input,output)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(train_X, train_Y,batch_size=512,epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Jddea9vdUhrs",
    "outputId": "95d61760-b12a-4122-df39-65ee5841de9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "229/229 [==============================] - 0s 516us/step\n"
     ]
    }
   ],
   "source": [
    "res = model.evaluate(test_X,test_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i6_F2oHMdUMN"
   },
   "source": [
    "Text Classification using Bidirectional LSTM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "id": "4nfAlnNNUpFb",
    "outputId": "142f75de-ede5-4402-8c3b-773c8f8b330a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gyanendro/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "914/914 [==============================] - 1s 2ms/step - loss: 0.6685 - accuracy: 0.6335\n",
      "Epoch 2/10\n",
      "914/914 [==============================] - 0s 442us/step - loss: 0.6080 - accuracy: 0.6915\n",
      "Epoch 3/10\n",
      "914/914 [==============================] - 0s 423us/step - loss: 0.5832 - accuracy: 0.6904\n",
      "Epoch 4/10\n",
      "914/914 [==============================] - 0s 341us/step - loss: 0.5784 - accuracy: 0.6915\n",
      "Epoch 5/10\n",
      "914/914 [==============================] - 0s 339us/step - loss: 0.5659 - accuracy: 0.6969\n",
      "Epoch 6/10\n",
      "914/914 [==============================] - 0s 337us/step - loss: 0.5478 - accuracy: 0.7265\n",
      "Epoch 7/10\n",
      "914/914 [==============================] - 0s 328us/step - loss: 0.5319 - accuracy: 0.7473\n",
      "Epoch 8/10\n",
      "914/914 [==============================] - 0s 432us/step - loss: 0.5224 - accuracy: 0.7637\n",
      "Epoch 9/10\n",
      "914/914 [==============================] - 0s 336us/step - loss: 0.5131 - accuracy: 0.7659\n",
      "Epoch 10/10\n",
      "914/914 [==============================] - 0s 335us/step - loss: 0.4983 - accuracy: 0.7746\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fc7fc076f50>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_size = 50\n",
    "input=Input(shape=(maxlen,))\n",
    "embedding = Embedding(len(word2id), embedding_size, input_length=maxlen, weights = [embeddings_matrix],trainable=True)(input)\n",
    "rnn = Bidirectional(LSTM(64,activation='tanh'))(embedding)\n",
    "output = Dense(2,activation='softmax')(rnn)\n",
    "model=Model(input,output)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(train_X, train_Y,batch_size=512,epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Syk8AQ9AUzpt",
    "outputId": "ad46c725-4fde-48d8-a393-ded6b33810e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "229/229 [==============================] - 0s 851us/step\n"
     ]
    }
   ],
   "source": [
    "res = model.evaluate(test_X,test_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a91TzU7SdaUd"
   },
   "source": [
    "# **Part II: Sequential Tagging**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Fti5j_kKU9lw",
    "outputId": "b2e09f78-ab54-43ca-e24b-6eab2de49c21"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "fp=open('Dataset/seq_tagging_dataset.txt')\n",
    "lines = fp.readlines()\n",
    "fp.close()\n",
    "print (len(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "dWw0zJ7EVAcN",
    "outputId": "6b8212c3-32cf-4e2f-cb9d-2c94f04a2d72"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "80\n"
     ]
    }
   ],
   "source": [
    "split_size = int(0.8*len(lines)) \n",
    "train_lines = lines[:split_size]\n",
    "test_lines = lines[split_size:]\n",
    "print (len(test_lines))\n",
    "print (len(train_lines))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jawt_xqDdfaw"
   },
   "source": [
    "Pre-processing modules for sequence tagging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "jgaIJ5wUVBc5",
    "outputId": "98004f51-182d-4ea4-9fad-c831798d0fd7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size :  1032\n",
      "Number of classes :  6\n",
      "Max sentence length :  80\n",
      "Set max length to 10 10\n"
     ]
    }
   ],
   "source": [
    "#assign unique ids to words and labels\n",
    "\n",
    "#first load all vocabulary and labels\n",
    "vocab = []\n",
    "classes = []\n",
    "maxlen = []\n",
    "for l in train_lines:\n",
    "  s = l.strip().split()\n",
    "  words = [w.strip().split('/')[0] for w in s]\n",
    "  labels = [w.strip().split('/')[1] for w in s]\n",
    "  vocab += words\n",
    "  classes += labels\n",
    "  maxlen.append(len(words))\n",
    "vocab = list(set(vocab))\n",
    "classes = list(set(classes))\n",
    "maxlen = len(maxlen)\n",
    "print (\"Vocabulary size : \", len(vocab))\n",
    "print (\"Number of classes : \", len(classes))\n",
    "print (\"Max sentence length : \", maxlen)\n",
    "maxlen = 10\n",
    "print (\"Set max length to 10\", maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "40lrb4EiVGOW"
   },
   "outputs": [],
   "source": [
    "#assin unique id to each word\n",
    "word2id = dict()\n",
    "id2word = dict()\n",
    "for i,v in enumerate(vocab,1):\n",
    "  word2id[v] = i\n",
    "  id2word[i] = v\n",
    "word2id['PAD'] = 0\n",
    "id2word[0] = 'PAD'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "WeJ9V36_VJCW",
    "outputId": "8611453f-633d-412c-dd11-72715afe3089"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary size :  1033\n"
     ]
    }
   ],
   "source": [
    "print (\"Dictionary size : \", len(word2id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "TfEHa2JTVMN-",
    "outputId": "4f67b74d-97d5-4d3d-aa1c-b467e2640ce8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class dictionary size :  6\n"
     ]
    }
   ],
   "source": [
    "#assign unique id to each class\n",
    "class2id = dict()\n",
    "id2class = dict()\n",
    "for i,c in enumerate(classes):\n",
    "  class2id[c] = i\n",
    "  id2class[i] = c\n",
    "print (\"Class dictionary size : \", len(class2id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Mqvr7E3aVM6j",
    "outputId": "6bc6b8ad-4648-4935-b02b-4d9f63a8db56"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80 80\n"
     ]
    }
   ],
   "source": [
    "#prepare training and test data\n",
    "#input : list of tokens\n",
    "#output : lits of labels corresponding to each token\n",
    "\n",
    "#training data preparation\n",
    "train_X = []\n",
    "train_Y = []\n",
    "for l in train_lines:\n",
    "  s = l.strip().split()\n",
    "  words = [w.strip().split('/')[0] for w in s]\n",
    "  labels = [w.strip().split('/')[1] for w in s]\n",
    "  words = words[:maxlen]\n",
    "  labels = labels[:maxlen]\n",
    "  temp_x = []\n",
    "  temp_y = []\n",
    "  temp_x = [word2id[w] if w in word2id else 0 for w in words]\n",
    "  temp_y = []\n",
    "  for label in labels:\n",
    "    y = [0]*len(class2id)\n",
    "    y[class2id[label]] = 1\n",
    "    temp_y.append(y)\n",
    "  padlen = maxlen - len(words)\n",
    "  temp_x += [0]*padlen\n",
    "  temp_y += ([[0]*len(class2id)])*padlen\n",
    "  train_X.append(temp_x)\n",
    "  train_Y.append(temp_y)\n",
    "print (len(train_X),len(train_Y))\n",
    "train_X = np.array(train_X)\n",
    "train_Y = np.array(train_Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "hTjiAhS-VbFR",
    "outputId": "e00a0413-df44-4c3b-8ae2-53832191917d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 20\n",
      "[0, 0, 154, 0, 445, 0, 198, 0, 0, 0] [[0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 1]]\n"
     ]
    }
   ],
   "source": [
    "#test data preparation\n",
    "test_X = []\n",
    "test_Y = []\n",
    "for l in test_lines:\n",
    "  s = l.strip().split()\n",
    "  words = [w.strip().split('/')[0] for w in s]\n",
    "  labels = [w.strip().split('/')[1] for w in s]\n",
    "  words = words[:maxlen]\n",
    "  labels = labels[:maxlen]\n",
    "  temp_x = []\n",
    "  temp_y = []\n",
    "  temp_x = [word2id[w] if w in word2id else 0 for w in words]\n",
    "  temp_y = []\n",
    "  for label in labels:\n",
    "    y = [0]*len(class2id)\n",
    "    y[class2id[label]] = 1\n",
    "    temp_y.append(y)\n",
    "  padlen = maxlen - len(words)\n",
    "  temp_x += [0]*padlen\n",
    "  temp_y += ([[0]*len(class2id)])*padlen\n",
    "  test_X.append(temp_x)\n",
    "  test_Y.append(temp_y)\n",
    "print (len(test_X),len(test_Y))\n",
    "print (test_X[0],test_Y[0])\n",
    "test_X = np.array(test_X)\n",
    "test_Y = np.array(test_Y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "toyVuADydmAo"
   },
   "source": [
    "Defining and Training a Sequential Tagging Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "Dw_3_-PEVeBd",
    "outputId": "0f9bd999-4766-4b76-d21d-7c2fb27ed951"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gyanendro/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "80/80 [==============================] - 1s 18ms/step - loss: 1.5393 - accuracy: 0.3913\n",
      "Epoch 2/5\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 1.4650 - accuracy: 0.4288\n",
      "Epoch 3/5\n",
      "80/80 [==============================] - 0s 982us/step - loss: 1.3040 - accuracy: 0.4288\n",
      "Epoch 4/5\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 1.1514 - accuracy: 0.4288\n",
      "Epoch 5/5\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 1.0880 - accuracy: 0.4288\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fc7f6d73690>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Define a model\n",
    "from keras.models import Model, Input\n",
    "from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\n",
    "input = Input(shape=(maxlen,))\n",
    "model = Embedding(input_dim=len(word2id), output_dim=50, input_length=maxlen)(input)  # 50-dim embedding\n",
    "model = Bidirectional(LSTM(units=100, return_sequences=True, recurrent_dropout=0.1))(model)\n",
    "out = TimeDistributed(Dense(len(class2id), activation=\"softmax\"))(model)\n",
    "model = Model(input, out)\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "model.fit(train_X, train_Y, batch_size=32, epochs=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "uP2jyGIRVhLj",
    "outputId": "526e598e-fc79-47c7-9d95-9f529a1dd200"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "20/20 [==============================] - 0s 17ms/step\n"
     ]
    }
   ],
   "source": [
    "res = model.evaluate(test_X,test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Nx8sRF3yVjM7"
   },
   "outputs": [],
   "source": [
    "p = model.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "colab_type": "code",
    "id": "F6e3QBthVpxn",
    "outputId": "c8d42362-9a07-4a87-d401-fdd04cf8843d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.15600762, 0.08643273, 0.05339229, 0.03950289, 0.24727088,\n",
       "        0.4173935 ],\n",
       "       [0.16425225, 0.10161273, 0.0692241 , 0.05476965, 0.2387815 ,\n",
       "        0.37135977],\n",
       "       [0.16236204, 0.10985445, 0.08220138, 0.06752402, 0.2342214 ,\n",
       "        0.34383672],\n",
       "       [0.16791405, 0.12003491, 0.09197135, 0.07857449, 0.22251144,\n",
       "        0.31899372],\n",
       "       [0.1577529 , 0.1199783 , 0.09640178, 0.08246656, 0.21797825,\n",
       "        0.32542217],\n",
       "       [0.15759075, 0.12238329, 0.09842084, 0.0842876 , 0.21450791,\n",
       "        0.32280964],\n",
       "       [0.13912931, 0.11425178, 0.09257907, 0.07742217, 0.21672826,\n",
       "        0.35988936],\n",
       "       [0.12731816, 0.10552424, 0.08256885, 0.06679167, 0.21888618,\n",
       "        0.39891085],\n",
       "       [0.10238122, 0.08877945, 0.06649007, 0.05044181, 0.21806842,\n",
       "        0.47383907],\n",
       "       [0.07080916, 0.06428806, 0.04472227, 0.03092928, 0.20671298,\n",
       "        0.5825382 ]], dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "LabDemo2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
